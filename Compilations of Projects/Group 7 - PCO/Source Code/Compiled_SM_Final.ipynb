{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca18399",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e39ec",
   "metadata": {},
   "source": [
    "Separate Data (MSM and SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d084853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"PCO.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Columns in your file:\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df[\"Headline\"] = df[\"Headline\"].fillna(\"\")\n",
    "df[\"Source\"] = df[\"Source\"].fillna(\"\").str.lower()\n",
    "sm_keywords = [\"facebook\", \"twitter\", \"instagram\", \"bsky\", \"youtube\", \"blogs\", \"reddit\", \"tiktok\"]\n",
    "\n",
    "force_msm_sources = [\n",
    "\n",
    "    \"germanexinthephilppines\", \"juancrisostomo\", \"wordpress\",\n",
    "\n",
    "    \"kwebanibarok\", \"majeca\", \"republikanews.org\", \"reyfortmedia\"\n",
    "\n",
    "]\n",
    "\n",
    "def classify_row(row):\n",
    "\n",
    "    source = row[\"Source\"]\n",
    "\n",
    "    headline = row[\"Headline\"]\n",
    "\n",
    "    if any(keyword in source for keyword in force_msm_sources):\n",
    "\n",
    "        return \"MSM\"\n",
    "\n",
    "    if headline.strip() == \"\":\n",
    "\n",
    "        return \"SM\"\n",
    "\n",
    "    if any(keyword in source for keyword in sm_keywords):\n",
    "\n",
    "        return \"SM\"\n",
    "\n",
    "    return \"MSM\"\n",
    "\n",
    "df[\"Category\"] = df.apply(classify_row, axis=1)\n",
    "\n",
    "msm_df = df[df[\"Category\"] == \"MSM\"]\n",
    "\n",
    "sm_df = df[df[\"Category\"] == \"SM\"]\n",
    "\n",
    "msm_df.to_csv(\"MSM.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "sm_df.to_csv(\"SM.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… Done! Saved {len(msm_df)} rows to MSM.csv and {len(sm_df)} rows to SM.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bf864",
   "metadata": {},
   "source": [
    "## 2. Clean SM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe477c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "file_path = \"SM.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_clean = df.drop_duplicates(subset=[\"Source Link\"], keep=\"first\")\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"Date\", \"Source\", \"Country\"])\n",
    "def fill_headline(row):\n",
    "\n",
    "    if pd.isna(row[\"Headline\"]) or str(row[\"Headline\"]).strip() == \"\":\n",
    "\n",
    "        if pd.notna(row.get(\"Opening Text\")) and str(row.get(\"Opening Text\")).strip() != \"\":\n",
    "\n",
    "            return row[\"Opening Text\"].strip()\n",
    "\n",
    "    return row[\"Headline\"]\n",
    "\n",
    "df_clean[\"Headline\"] = df_clean.apply(fill_headline, axis=1)\n",
    "\n",
    "def parse_date(row):\n",
    "\n",
    "    date_str = row[\"Date\"] if pd.notnull(row[\"Date\"]) else row.get(\"Alternate Date Format\", None)\n",
    "\n",
    "    try:\n",
    "\n",
    "        parsed = pd.to_datetime(date_str, errors=\"coerce\")\n",
    "\n",
    "        return parsed.date() if pd.notnull(parsed) else pd.NaT\n",
    "    except:\n",
    "\n",
    "        return pd.NaT\n",
    "\n",
    "df_clean.loc[:, \"Clean Date\"] = df_clean.apply(parse_date, axis=1)\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"Clean Date\"])\n",
    "\n",
    "df_clean.loc[:, \"Clean Date\"] = pd.to_datetime(df_clean[\"Clean Date\"]).dt.strftime(\"%m-%d-%Y\")\n",
    "def normalize_source(source):\n",
    "\n",
    "    s = str(source).strip().lower()\n",
    "\n",
    "    s = re.sub(r\"www\\.\", \"\", s)\n",
    "    s = re.sub(r\"\\.com|\\.ph|\\.net|\\.org\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    if \"reddit\" in s:\n",
    "\n",
    "        return \"Forums\"\n",
    "\n",
    "    return s.title()\n",
    "\n",
    "df_clean.loc[:, \"Source\"] = df_clean[\"Source\"].apply(normalize_source)\n",
    "\n",
    "def normalize_headline(text):\n",
    "\n",
    "    if pd.isnull(text):\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df_clean[\"headline_key\"] = df_clean[\"Headline\"].apply(normalize_headline)\n",
    "\n",
    "df_clean = df_clean.drop_duplicates(subset=[\"headline_key\"], keep=\"first\")\n",
    "\n",
    "df_clean = df_clean.drop(columns=[\"headline_key\"])\n",
    "\n",
    "for col in [\"Opening Text\", \"Influencer\", \"Key Phrases\", \"Language\", \"Sentiment\"]:\n",
    "\n",
    "    if col in df_clean.columns:\n",
    "\n",
    "        df_clean[col] = df_clean[col].fillna(\"Unknown\")\n",
    "        df_clean[col] = df_clean[col].replace(r\"^\\s*$\", \"Unknown\", regex=True)\n",
    "social_echo_cols = [\"Twitter Social Echo\", \"Facebook Social Echo\", \"Reddit Social Echo\"]\n",
    "\n",
    "cols_to_fix = social_echo_cols + [\"Reach\"]\n",
    "\n",
    "for col in cols_to_fix:\n",
    "\n",
    "    if col in df_clean.columns:\n",
    "\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "if all(col in df_clean.columns for col in social_echo_cols):\n",
    "\n",
    "    df_clean[\"Social Echo Total\"] = (\n",
    "\n",
    "        df_clean[\"Twitter Social Echo\"]\n",
    "\n",
    "        + df_clean[\"Facebook Social Echo\"]\n",
    "\n",
    "        + df_clean[\"Reddit Social Echo\"]\n",
    "\n",
    "    ).astype(int)\n",
    "\n",
    "elif \"Social Echo Total\" in df_clean.columns:\n",
    "\n",
    "    df_clean[\"Social Echo Total\"] = pd.to_numeric(df_clean[\"Social Echo Total\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "df_clean = df_clean[~df_clean[\"Headline\"].str.contains(\"content from this publisher\", case=False, na=False)]\n",
    "\n",
    "df_clean = df_clean[~df_clean[\"Source Link\"].str.contains(\"proquest\", case=False, na=False)]\n",
    "\n",
    "df_clean = df_clean[~df_clean[\"Headline\"].str.contains(\"test\", case=False, na=False)]\n",
    "\n",
    "df_clean = df_clean[df_clean[\"Source Link\"].str.strip() != \"\"]\n",
    "\n",
    "if \"Hit Sentence\" in df_clean.columns:\n",
    "\n",
    "    df_clean = df_clean[~df_clean[\"Hit Sentence\"].str.contains(r\"\\[Courtesy:|\\[Photo courtesy\\]\", case=False, na=False)]\n",
    "\n",
    "cols_to_drop = [\n",
    "\n",
    "    \"Subregion\", \"Desktop Reach\", \"Mobile Reach\",\n",
    "\n",
    "    \"National Viewership\", \"Engagement\", \"AVE\", \"Twitter Authority\",\n",
    "\n",
    "    \"Tweet Id\", \"Twitter Id\", \"Twitter Client\", \"Twitter Screen Name\",\n",
    "\n",
    "    \"User Profile Url\", \"Twitter Bio\", \"Twitter Followers\", \"Twitter Following\",\n",
    "\n",
    "    \"Alternate Date Format\", \"Time\", \"State\", \"City\", \"Editorial Echo\",\n",
    "\n",
    "    \"Views\", \"Estimated Views\", \"Likes\", \"Replies\", \"Retweets\",\n",
    "\n",
    "    \"Comments\", \"Shares\", \"Reactions\", \"Threads\", \"Is Verified\",\n",
    "\n",
    "    \"Parent URL\", \"Document Tags\", \"Document ID\", \"Custom Categories\"\n",
    "\n",
    "]\n",
    "\n",
    "df_clean = df_clean.drop(columns=[c for c in cols_to_drop if c in df_clean.columns])\n",
    "\n",
    "if \"Country\" in df_clean.columns:\n",
    "\n",
    "    df_clean[\"Country\"] = df_clean[\"Country\"].astype(str).str.strip().str.title()\n",
    "\n",
    "if \"Headline\" in df_clean.columns:\n",
    "\n",
    "    df_clean[\"Headline\"] = df_clean[\"Headline\"].astype(str).str.strip()\n",
    "\n",
    "    df_clean[\"Headline\"] = df_clean[\"Headline\"].replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    df_clean[\"Headline\"] = df_clean[\"Headline\"].str.title()\n",
    "text_cols = [\"Opening Text\", \"Hit Sentence\"]\n",
    "\n",
    "for col in text_cols:\n",
    "\n",
    "    if col in df_clean.columns:\n",
    "\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "\n",
    "        df_clean[col] = df_clean[col].replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "        df_clean[col] = df_clean[col].str.capitalize()\n",
    "\n",
    "\n",
    "if \"Clean Date\" in df_clean.columns:\n",
    "    cols = [\"Clean Date\"] + [c for c in df_clean.columns if c != \"Clean Date\"]\n",
    "    df_clean = df_clean[cols]\n",
    "\n",
    "# addition for standardization and new conditions for cleaning - updated September 2025\n",
    "def standardize_keyphrases(text):\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    text = str(text)\n",
    "    parts = re.split(r'[;,|/â€¢]+', text)\n",
    "    normalized = []\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if p == \"\":\n",
    "            continue\n",
    "        low = p.lower()\n",
    "        if \"pco\" in low or \"presidential communications\" in low:\n",
    "            normalized.append(\"Presidential Communications Office\")\n",
    "        elif \"philippines\" in low or \"pilipinas\" in low or \"pilipino\" in low:\n",
    "            normalized.append(\"Philippines\")\n",
    "        else:\n",
    "            normalized.append(p.title())\n",
    "    seen = set()\n",
    "    final_list = []\n",
    "    for item in normalized:\n",
    "        if item not in seen:\n",
    "            final_list.append(item)\n",
    "            seen.add(item)\n",
    "    return \", \".join(final_list) if final_list else \"Unknown\"\n",
    "\n",
    "if \"Key Phrases\" in df_clean.columns:\n",
    "    df_clean[\"Key Phrases\"] = df_clean[\"Key Phrases\"].apply(standardize_keyphrases)\n",
    "\n",
    "if \"Keywords\" in df_clean.columns:\n",
    "    df_clean[\"Keywords\"] = df_clean[\"Keywords\"].apply(standardize_keyphrases)\n",
    "\n",
    "def detect_language_for_country(text):\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "def map_language_to_country(lang_code):\n",
    "    mapping = {\n",
    "        \"tl\": \"Philippines\",\n",
    "        \"fil\": \"Philippines\",\n",
    "        \"en\": \"United States\",\n",
    "        \"ja\": \"Japan\",\n",
    "        \"ko\": \"South Korea\",\n",
    "        \"zh\": \"China\",\n",
    "        \"fr\": \"France\",\n",
    "        \"es\": \"Spain\",\n",
    "        \"de\": \"Germany\",\n",
    "        \"ms\": \"Malaysia\",\n",
    "        \"id\": \"Indonesia\",\n",
    "    }\n",
    "    return mapping.get(lang_code, None)\n",
    "\n",
    "if \"Country\" in df_clean.columns and \"Hit Sentence\" in df_clean.columns:\n",
    "    def fill_country(row):\n",
    "        country = str(row[\"Country\"]).strip().lower()\n",
    "        if country == \"\" or country == \"unknown\" or country == \"nan\":\n",
    "            lang_code = detect_language_for_country(row[\"Hit Sentence\"])\n",
    "            inferred_country = map_language_to_country(lang_code)\n",
    "            return inferred_country if inferred_country else \"Unknown\"\n",
    "        return row[\"Country\"]\n",
    "\n",
    "    df_clean[\"Country\"] = df_clean.apply(fill_country, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def detect_tl_or_en(text):\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return \"English\"\n",
    "    try:\n",
    "        lang_code = detect(str(text))\n",
    "    except LangDetectException:\n",
    "        return \"English\"\n",
    "\n",
    "    if lang_code in [\"tl\", \"fil\"]:\n",
    "        return \"Tagalog\"\n",
    "    else:\n",
    "        return \"English\"\n",
    "\n",
    "if \"Language\" in df_clean.columns and \"Hit Sentence\" in df_clean.columns:\n",
    "    def fill_language(row):\n",
    "        lang = str(row[\"Language\"]).strip().lower()\n",
    "        if lang == \"\" or lang == \"unknown\" or lang == \"nan\":\n",
    "            return detect_tl_or_en(row[\"Hit Sentence\"])\n",
    "        return row[\"Language\"]\n",
    "\n",
    "    df_clean[\"Language\"] = df_clean.apply(fill_language, axis=1)\n",
    "\n",
    "\n",
    "output_path = \"SM_cleaned.xlsx\"\n",
    "df_clean.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Cleaning complete!\")\n",
    "print(f\"Remaining rows: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce9df5",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26078247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ae5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('SM_cleaned.xlsx')\n",
    "\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"SM_cleaned.xlsx\"\n",
    "df_cleaned = pd.read_excel(file_path)\n",
    "\n",
    "print(\"âœ… Original Sentiment Distribution from SM_cleaned.xlsx:\\n\")\n",
    "\n",
    "print(df_cleaned['Sentiment'].value_counts(dropna=False))\n",
    "\n",
    "rows_with_sentiment = df_cleaned['Sentiment'].notna().sum()\n",
    "rows_without_sentiment = df_cleaned['Sentiment'].isna().sum()\n",
    "\n",
    "print(f\"\\nRows with sentiment: {rows_with_sentiment}\")\n",
    "print(f\"Rows without sentiment: {rows_without_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "file_path = \"SM_cleaned.xlsx\"\n",
    "df_cleaned = pd.read_excel(file_path)\n",
    "\n",
    "def get_new_sentiment(text):\n",
    "\n",
    "    if pd.isnull(text):\n",
    "\n",
    "        return 'Neutral'\n",
    "\n",
    "    polarity = TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "    if polarity > 0.1:\n",
    "\n",
    "        return 'Positive'\n",
    "\n",
    "    elif polarity < -0.1:\n",
    "\n",
    "        return 'Negative'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Neutral'\n",
    "\n",
    "df['New_Sentiment'] = df['Headline'].apply(get_new_sentiment)\n",
    "\n",
    "print('New Sentiment Distribution:')\n",
    "\n",
    "print(df['New_Sentiment'].value_counts())\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c286d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = \"SM_cleaned.xlsx\"\n",
    "df_cleaned = pd.read_excel(file_path)\n",
    "\n",
    "sentiment_counts = df_cleaned['Sentiment'].value_counts()\n",
    "\n",
    "color_map = {\n",
    "    \"Positive\": \"green\",\n",
    "    \"Negative\": \"red\",\n",
    "    \"Neutral\": \"gray\",\n",
    "    \"Unknown\": \"orange\"\n",
    "}\n",
    "colors = [color_map.get(s, \"blue\") for s in sentiment_counts.index]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sentiment_counts.plot(kind='bar', color=colors)\n",
    "\n",
    "plt.title(\"Original Sentiment Distribution\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sentiment\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    ax.text(i, v + max(sentiment_counts) * 0.01, str(v),\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = \"SM_cleaned.xlsx\"\n",
    "df_cleaned = pd.read_excel(file_path)\n",
    "\n",
    "if \"New_Sentiment\" not in df_cleaned.columns:\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    def get_new_sentiment(text):\n",
    "        if pd.isnull(text):\n",
    "            return 'Neutral'\n",
    "        polarity = TextBlob(str(text)).sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            return 'Positive'\n",
    "        elif polarity < -0.1:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "    df_cleaned['New_Sentiment'] = df_cleaned['Headline'].apply(get_new_sentiment)\n",
    "\n",
    "new_sentiment_counts = df_cleaned['New_Sentiment'].value_counts()\n",
    "\n",
    "color_map = {\n",
    "    \"Positive\": \"green\",\n",
    "    \"Negative\": \"red\",\n",
    "    \"Neutral\": \"gray\",\n",
    "    \"Unknown\": \"orange\"\n",
    "}\n",
    "colors = [color_map.get(s, \"blue\") for s in new_sentiment_counts.index]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = new_sentiment_counts.plot(kind='bar', color=colors)\n",
    "\n",
    "plt.title(\"New Sentiment Distribution\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sentiment\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, v in enumerate(new_sentiment_counts):\n",
    "    ax.text(i, v + max(new_sentiment_counts) * 0.01, str(v),\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdd9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "file_path = \"SM_cleaned.xlsx\"\n",
    "df_cleaned = pd.read_excel(file_path)\n",
    "\n",
    "if \"New_Sentiment\" not in df_cleaned.columns:\n",
    "    def get_new_sentiment(text):\n",
    "        if pd.isnull(text):\n",
    "            return 'Neutral'\n",
    "        polarity = TextBlob(str(text)).sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            return 'Positive'\n",
    "        elif polarity < -0.1:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "    df_cleaned['New_Sentiment'] = df_cleaned['Headline'].apply(get_new_sentiment)\n",
    "\n",
    "diff_sentiment_samples = df_cleaned[df_cleaned['Sentiment'] != df_cleaned['New_Sentiment']]\n",
    "\n",
    "sample_count = min(5, len(diff_sentiment_samples))\n",
    "diff_sentiment_samples = diff_sentiment_samples.sample(n=sample_count, random_state=42)\n",
    "\n",
    "print(f\"ðŸ§ª Showing {sample_count} examples where the sentiment changed:\\n\")\n",
    "for idx, row in diff_sentiment_samples.iterrows():\n",
    "    print(f\"Headline: {row['Headline']}\")\n",
    "    print(f\"Original Sentiment: {row['Sentiment']}\")\n",
    "    print(f\"New Sentiment: {row['New_Sentiment']}\")\n",
    "    print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ff3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'sentiment_analysis_results.xlsx'\n",
    "\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Excel file '{output_file}' has been created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a54db",
   "metadata": {},
   "source": [
    "## 4. Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8998bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "df = pd.read_excel('sentiment_analysis_results.xlsx')\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0967dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentiment_analysis_results.xlsx\"\n",
    "df_results = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "df_results.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentiment_analysis_results.xlsx\"\n",
    "df_results = pd.read_excel(file_path)\n",
    "\n",
    "daily_volume = (\n",
    "    df.groupby('Clean Date')\n",
    "      .size()\n",
    "      .reset_index(name='Story_Count')\n",
    "      .sort_values('Clean Date')\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.plot(\n",
    "    daily_volume['Clean Date'],\n",
    "    daily_volume['Story_Count'],\n",
    "    color='purple',\n",
    "    linewidth=2,\n",
    "    marker='o',\n",
    "    markersize=4,\n",
    "    alpha=0.8,\n",
    "    label='Story Volume'\n",
    ")\n",
    "\n",
    "ax.set_title(\"Daily Story Volume â€“ Trendline\", fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of Stories\", fontsize=12)\n",
    "\n",
    "ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32510789",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentiment_analysis_results.xlsx\"\n",
    "df_results = pd.read_excel(file_path)\n",
    "\n",
    "if \"New_Sentiment\" not in df_results.columns:\n",
    "    raise ValueError(\"âš ï¸ 'New_Sentiment' column not found in sentiment_analysis_results.xlsx\")\n",
    "\n",
    "sentiment_counts = df_results['New_Sentiment'].value_counts()\n",
    "\n",
    "color_map = {\n",
    "    \"Positive\": \"green\",\n",
    "    \"Negative\": \"red\",\n",
    "    \"Neutral\": \"gray\",\n",
    "    \"Unknown\": \"orange\"\n",
    "}\n",
    "\n",
    "colors = [color_map.get(label, \"blue\") for label in sentiment_counts.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.pie(\n",
    "    sentiment_counts.values,\n",
    "    labels=sentiment_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=140\n",
    ")\n",
    "\n",
    "ax.set_title('Overall Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa848fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentiment_analysis_results.xlsx\"\n",
    "df_results = pd.read_excel(file_path)\n",
    "\n",
    "top_sources = df['Source'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_sources.sort_values().plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 Sources by Story Count', fontsize=14)\n",
    "plt.xlabel('Number of Stories')\n",
    "plt.ylabel('Source')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sentiment_analysis_results.xlsx\"\n",
    "df_results = pd.read_excel(file_path)\n",
    "\n",
    "top_countries = df['Country'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_countries.sort_values().plot(kind='barh', color='coral')\n",
    "plt.title('Top 10 Countries by Story Count', fontsize=14)\n",
    "plt.xlabel('Number of Stories')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = 'sentiment_analysis_results.xlsx.'\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "df['Clean Date'] = pd.to_datetime(df['Clean Date'], errors='coerce')\n",
    "\n",
    "df['Reach'] = pd.to_numeric(df['Reach'], errors='coerce').fillna(0)\n",
    "\n",
    "daily_reach = df.groupby('Clean Date')['Reach'].sum().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(daily_reach['Clean Date'], daily_reach['Reach'], linewidth=2, color='purple')\n",
    "ax.set_title('Daily Reach - Trendline', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Reach')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = 'sentiment_analysis_results.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "df['Total_Social_Echo'] = pd.to_numeric(df['Social Echo Total'], errors='coerce').fillna(0)\n",
    "\n",
    "df['Clean Date'] = pd.to_datetime(df['Clean Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "daily_social_echo = df.groupby('Clean Date')['Total_Social_Echo'].sum().reset_index()\n",
    "daily_social_echo = daily_social_echo.sort_values('Clean Date')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(\n",
    "    daily_social_echo['Clean Date'],\n",
    "    daily_social_echo['Total_Social_Echo'],\n",
    "    linewidth=2,\n",
    "    color='purple'\n",
    ")\n",
    "\n",
    "ax.set_title('Daily Social Echo - Trendline', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Total Social Echo')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c57ab5",
   "metadata": {},
   "source": [
    "## 5. Challenge 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efc881",
   "metadata": {},
   "source": [
    "Code to Identify the Top 10 Keywords via Keywords Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('sentiment_analysis_results.xlsx.')\n",
    "\n",
    "print(\"Analyzing Keywords...\")\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for keywords in df['Keywords'].dropna():\n",
    "\n",
    "    if ',' in str(keywords):\n",
    "\n",
    "        keyword_list = str(keywords).split(',')\n",
    "\n",
    "    elif ';' in str(keywords):\n",
    "\n",
    "        keyword_list = str(keywords).split(';')\n",
    "\n",
    "    else:\n",
    "\n",
    "        keyword_list = [str(keywords)]\n",
    "\n",
    "    all_keywords.extend([k.strip() for k in keyword_list if k.strip()])\n",
    "\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "top_keywords = keyword_counts.most_common(10)\n",
    "\n",
    "print(f\"Total unique keywords: {len(keyword_counts)}\")\n",
    "\n",
    "print(f\"Total keyword occurrences: {len(all_keywords)}\")\n",
    "\n",
    "print(\"\\nTop 10 Keywords:\")\n",
    "\n",
    "for keyword, count in top_keywords:\n",
    "\n",
    "    print(f\"{keyword}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6843171",
   "metadata": {},
   "source": [
    "Code to Identify the Top 10 Phrases via Key Phrases Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3adbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('sentiment_analysis_results.xlsx.')\n",
    "\n",
    "print(\"\\nAnalyzing Key Phrases...\")\n",
    "\n",
    "all_phrases = []\n",
    "\n",
    "for phrases in df['Key Phrases'].dropna():\n",
    "\n",
    "    if ',' in str(phrases):\n",
    "\n",
    "        phrase_list = str(phrases).split(',')\n",
    "\n",
    "    elif ';' in str(phrases):\n",
    "\n",
    "        phrase_list = str(phrases).split(';')\n",
    "\n",
    "    else:\n",
    "\n",
    "        phrase_list = [str(phrases)]\n",
    "\n",
    "    all_phrases.extend([p.strip() for p in phrase_list if p.strip()])\n",
    "\n",
    "phrase_counts = Counter(all_phrases)\n",
    "\n",
    "top_phrases = phrase_counts.most_common(10)\n",
    "\n",
    "print(f\"Total unique phrases: {len(phrase_counts)}\")\n",
    "\n",
    "print(f\"Total phrase occurrences: {len(all_phrases)}\")\n",
    "\n",
    "print(\"\\nTop 10 Key Phrases:\")\n",
    "\n",
    "for phrase, count in top_phrases:\n",
    "\n",
    "    print(f\"{phrase}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a44641",
   "metadata": {},
   "source": [
    "Code to Identify the Top 10 Keywords via Keywords Column (VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8087d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "keywords_df = pd.DataFrame(top_keywords, columns=['Keyword', 'Count'])\n",
    "keywords_df['Keyword'] = keywords_df['Keyword'].str.replace(' ', '\\n')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "bars = keywords_df.plot(\n",
    "    x='Keyword', \n",
    "    y='Count', \n",
    "    kind='bar', \n",
    "    ax=ax, \n",
    "    color='#2D3289', \n",
    "    legend=False\n",
    ")\n",
    "\n",
    "ax.set_facecolor('#FFFFFF')\n",
    "ax.set_title('Top Keywords', fontsize=12, color='#2D3289', fontweight='bold')\n",
    "ax.set_xlabel('Keyword', fontsize=10, color='#2D3289')\n",
    "ax.set_ylabel('Frequency', fontsize=10, color='#2D3289')\n",
    "ax.tick_params(rotation=0, axis='x', labelsize=8)\n",
    "ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    ax.text(\n",
    "        patch.get_x() + patch.get_width() / 2,\n",
    "        height + max(keywords_df['Count']) * 0.01,  \n",
    "        f\"{int(height)}\", \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=9, \n",
    "        fontweight='bold',\n",
    "        color='#2D3289'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d471d",
   "metadata": {},
   "source": [
    "Code to Identify the Top 10 Phrases via Key Phrases Column (VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "phrases_df = pd.DataFrame(top_phrases, columns=['Phrase', 'Count'])\n",
    "phrases_df = phrases_df[~phrases_df['Phrase'].isin(['Uncategorized', 'Unknown', 'N/A'])]\n",
    "\n",
    "phrases_df = phrases_df.head(10)\n",
    "\n",
    "phrases_df['Phrase'] = phrases_df['Phrase'].str.replace(' ', '\\n')\n",
    "\n",
    "# Plot\n",
    "phrases_df.plot(x='Phrase', y='Count', kind='bar', ax=ax, color='#2D3289')\n",
    "ax.set_title('Top 10 Key Phrases (excluding Uncategorized/Unknown)', fontsize=12, color='#2D3289', fontweight='bold')\n",
    "ax.set_xlabel('Key Phrase', fontsize=10, color='#2D3289')\n",
    "ax.set_ylabel('Frequency', fontsize=10, color='#2D3289')\n",
    "ax.tick_params(rotation=0, axis='x', labelsize=8)\n",
    "ax.tick_params(axis='y', labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7754e3",
   "metadata": {},
   "source": [
    "Code to Identify the Top 10 Keywords in the Headline via Headline Column with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "print(\"Analyzing Keywords...\")\n",
    "all_keywords = []\n",
    "for keywords in df['Headline'].dropna():\n",
    "    if ',' in str(keywords):\n",
    "        keyword_list = str(keywords).split(',')\n",
    "    elif ';' in str(keywords):\n",
    "        keyword_list = str(keywords).split(';')\n",
    "    else:\n",
    "        keyword_list = [str(keywords)]\n",
    "    all_keywords.extend([k.strip() for k in keyword_list if k.strip()])\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = keyword_counts.most_common(10)\n",
    "print(f\"Total unique keywords in Headline: {len(keyword_counts)}\")\n",
    "print(f\"Total keyword occurrences in Headline: {len(all_keywords)}\")\n",
    "print(\"\\nTop 10 Keywords in Headline:\")\n",
    "for keyword, count in top_keywords:\n",
    "    print(f\"{keyword}: {count}\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "keywords_df = pd.DataFrame(top_keywords, columns=['Keyword', 'Count'])\n",
    "keywords_df['Keyword'] = keywords_df['Keyword'].str.replace(' ', '\\n')\n",
    "keywords_df.plot(x='Keyword', y='Count', kind='bar', ax=ax, color='#2D3289')\n",
    "ax.set_facecolor('#FFFFFF')\n",
    "ax.set_title('Top 10 Keywords in Headline', fontsize=12, color='#2D3289', fontweight='bold')\n",
    "ax.set_xlabel('Keyword', fontsize=10, color='#2D3289')\n",
    "ax.set_ylabel('Frequency', fontsize=10, color='#2D3289')\n",
    "ax.tick_params(rotation=0, axis='x', labelsize=8,)\n",
    "ax.tick_params(axis='y', labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f85ad",
   "metadata": {},
   "source": [
    "Code to Identify the Top 3 Keywords within the Top 5 Sources via Keywords & Sources Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "print(\"\\nKeyword Usage by Source:\")\n",
    "\n",
    "source_keywords = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    source = row['Source']\n",
    "\n",
    "    keywords_str = row['Keywords']\n",
    "\n",
    "    if pd.notna(keywords_str) and pd.notna(source):\n",
    "\n",
    "        if ',' in str(keywords_str):\n",
    "\n",
    "            keywords = [k.strip() for k in str(keywords_str).split(',')]\n",
    "\n",
    "        elif ';' in str(keywords_str):\n",
    "\n",
    "            keywords = [k.strip() for k in str(keywords_str).split(';')]\n",
    "\n",
    "        else:\n",
    "\n",
    "            keywords = [str(keywords_str).strip()]\n",
    "\n",
    "        if source not in source_keywords:\n",
    "\n",
    "            source_keywords[source] = []\n",
    "\n",
    "        source_keywords[source].extend(keywords)\n",
    "\n",
    "top_5_sources = df['Source'].value_counts().head(5).index\n",
    "\n",
    "print(\"\\nTop Keywords by Top 5 Sources:\")\n",
    "\n",
    "for source in top_5_sources:\n",
    "\n",
    "    if source in source_keywords:\n",
    "\n",
    "        source_keyword_counts = Counter(source_keywords[source])\n",
    "\n",
    "        top_3_keywords = source_keyword_counts.most_common(3)\n",
    "\n",
    "        print(f\"\\n{source}:\")\n",
    "\n",
    "        for keyword, count in top_3_keywords:\n",
    "\n",
    "            print(f\"  - {keyword}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e07e8b",
   "metadata": {},
   "source": [
    "Code to Identify the Top 3 Key Phrases within the Top 5 Sources via Key Phrases & Sources Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5048a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "print(\"\\nTop Phrases by Top 5 Sources:\")\n",
    "\n",
    "top_5_sources = df['Source'].value_counts().head(5).index\n",
    "\n",
    "source_phrases = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    source = row['Source']\n",
    "\n",
    "    phrases_str = row['Key Phrases']\n",
    "\n",
    "    if pd.notna(source) and source in top_5_sources and pd.notna(phrases_str):\n",
    "\n",
    "        if ',' in phrases_str:\n",
    "\n",
    "            phrases = [p.strip() for p in phrases_str.split(',')]\n",
    "\n",
    "        elif ';' in phrases_str:\n",
    "\n",
    "            phrases = [p.strip() for p in phrases_str.split(';')]\n",
    "\n",
    "        else:\n",
    "\n",
    "            phrases = [phrases_str.strip()]\n",
    "\n",
    "        if source not in source_phrases:\n",
    "\n",
    "            source_phrases[source] = []\n",
    "\n",
    "        source_phrases[source].extend(phrases)\n",
    "\n",
    "for source in top_5_sources:\n",
    "\n",
    "    phrases = source_phrases.get(source, [])\n",
    "\n",
    "    phrase_counts = Counter(phrases)\n",
    "\n",
    "    top_phrases = phrase_counts.most_common(3)  \n",
    "\n",
    "    print(f\"\\n{source}:\")\n",
    "\n",
    "    for phrase, count in top_phrases:\n",
    "\n",
    "        print(f\"  - {phrase}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('SM_cleaned.xlsx')\n",
    "df2 = pd.read_excel('sentiment_analysis_results.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e5644",
   "metadata": {},
   "source": [
    "Top Keywords by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baff870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "print(\"\\nTop Keywords by Sentiment:\")\n",
    "\n",
    "sentiments = df2['New_Sentiment'].dropna().unique()\n",
    "\n",
    "sentiment_keywords = {}\n",
    "\n",
    "for idx, row in df2.iterrows():\n",
    "    sentiment = row['New_Sentiment']\n",
    "    keywords_str = row['Keywords']\n",
    "    \n",
    "    if pd.notna(sentiment) and pd.notna(keywords_str):\n",
    "        if ',' in keywords_str:\n",
    "            keywords = [p.strip() for p in keywords_str.split(',')]\n",
    "        elif ';' in keywords_str:\n",
    "            keywords = [p.strip() for p in keywords_str.split(';')]\n",
    "        else:\n",
    "            keywords = [keywords_str.strip()]\n",
    "        \n",
    "        if sentiment not in sentiment_keywords:\n",
    "            sentiment_keywords[sentiment] = []\n",
    "        sentiment_keywords[sentiment].extend(keywords)\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    keywords = sentiment_keywords.get(sentiment, [])\n",
    "    keyword_counts = Counter(keywords)\n",
    "    top_keywords = keyword_counts.most_common(3) \n",
    "    \n",
    "    print(f\"\\n{sentiment}:\")\n",
    "    for keyword, count in top_keywords:\n",
    "        print(f\"  - {keyword}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42177090",
   "metadata": {},
   "source": [
    "Top Key Phrases by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b897b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "print(\"\\nTop Key Phrases by Sentiment:\")\n",
    "\n",
    "sentiments = df2['New_Sentiment'].dropna().unique()\n",
    "\n",
    "sentiment_phrases = {}\n",
    "\n",
    "for idx, row in df2.iterrows():\n",
    "    sentiment = row['New_Sentiment']\n",
    "    phrases_str = row['Key Phrases']\n",
    "    \n",
    "    if pd.notna(sentiment) and pd.notna(phrases_str):\n",
    "        if ',' in phrases_str:\n",
    "            phrases = [p.strip() for p in phrases_str.split(',')]\n",
    "        elif ';' in phrases_str:\n",
    "            phrases = [p.strip() for p in phrases_str.split(';')]\n",
    "        else:\n",
    "            phrases = [phrases_str.strip()]\n",
    "        \n",
    "        if sentiment not in sentiment_phrases:\n",
    "            sentiment_phrases[sentiment] = []\n",
    "        sentiment_phrases[sentiment].extend(phrases)\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    phrases = sentiment_phrases.get(sentiment, [])\n",
    "    phrase_counts = Counter(phrases)\n",
    "    top_phrases = phrase_counts.most_common(3)\n",
    "    \n",
    "    print(f\"\\n{sentiment}:\")\n",
    "    for phrase, count in top_phrases:\n",
    "        print(f\"  - {phrase}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5257c",
   "metadata": {},
   "source": [
    "## 6. Challenge 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f3c01",
   "metadata": {},
   "source": [
    "Word cloud from Key Phrases, Keywords, and extracted topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40107a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "all_text_keywords = ' '.join(map(str, all_keywords))\n",
    "\n",
    "try:\n",
    "    all_text_phrases = ' '.join(map(str, all_phrases))\n",
    "except NameError:\n",
    "    all_text_phrases = ''\n",
    "\n",
    "combined_text = (all_text_keywords + ' ' + all_text_phrases).strip()\n",
    "\n",
    "if combined_text:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                         colormap='viridis', max_words=100).generate(combined_text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Overall Word Cloud - All Keywords and Phrases', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available to generate a word cloud.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b2f1a",
   "metadata": {},
   "source": [
    "Generate separate clouds for different sentiment categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df = pd.read_excel(\"sentiment_analysis_results.xlsx\")\n",
    "\n",
    "sentiment_column = \"New_Sentiment\" if \"New_Sentiment\" in df.columns else \"Sentiment\"\n",
    "\n",
    "colormaps = {\n",
    "    \"Positive\": \"Greens\",\n",
    "    \"Negative\": \"Reds\",\n",
    "    \"Neutral\": \"Blues\"\n",
    "}\n",
    "\n",
    "sentiments = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "for ax, sentiment in zip(axes, sentiments):\n",
    "    sentiment_df = df[df[sentiment_column].str.lower() == sentiment.lower()]\n",
    "\n",
    "    all_text_keywords = ' '.join(map(str, sentiment_df[\"Keywords\"]))\n",
    "    all_text_phrases = ' '.join(map(str, sentiment_df[\"Key Phrases\"]))\n",
    "    combined_text = (all_text_keywords + ' ' + all_text_phrases).strip()\n",
    "\n",
    "    if combined_text:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                              colormap=colormaps[sentiment], max_words=100).generate(combined_text)\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title(f'{sentiment} Sentiment Word Cloud', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"No data for {sentiment}\", ha='center', va='center', fontsize=14)\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683afdd7",
   "metadata": {},
   "source": [
    "Create time-based word clouds (monthly or quarterly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46020e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('sentiment_analysis_results.xlsx')\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55068a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "def get_week_of_month(date):\n",
    "    first_day = date.replace(day=1)\n",
    "    dom = date.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "    return int(np.ceil(adjusted_dom/7.0))\n",
    "\n",
    "df.insert(df.columns.get_loc('Date') + 1, 'Week', df['Date'].apply(get_week_of_month))\n",
    "\n",
    "print(f\"New dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nFirst five rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = df['Keywords'].dropna().astype(str).tolist()\n",
    "all_phrases = df['Key Phrases'].dropna().astype(str).tolist()\n",
    "all_text_keywords = ' '.join(all_keywords)\n",
    "all_text_phrases = ' '.join(all_phrases)\n",
    "combined_text = all_text_keywords + ' ' + all_text_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = sorted(df['Week'].unique())\n",
    "\n",
    "for week in weeks:\n",
    "    week_df = df[df['Week'] == week]\n",
    "    all_keywords = week_df['Keywords'].dropna().astype(str).tolist()\n",
    "    all_phrases = week_df['Key Phrases'].dropna().astype(str).tolist()\n",
    "    combined_text = ' '.join(all_keywords) + ' ' + ' '.join(all_phrases)\n",
    "    if combined_text.strip():\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for Week {week}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'No data for week {week}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f276446",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'FINAL_SM.xlsx'\n",
    "\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Excel file '{output_file}' has been created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
