{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HIBo_x_T2HpZ-R9hHXcYdJqZnkoyfKw6","timestamp":1758525047159}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Important Links**\n","\n","ðŸ”—[Understanding some keywords found in this notebook](https://docs.google.com/document/d/1Ms0QsfVngobfWqt56lBRlsSpXwqDYske70nw33osBbk/edit?tab=t.0#bookmark=id.6pcek6ioj1hn)\n","\n","*The keywords and linked guide above is important to ensure the codes run when tested from the backend.*\n","\n","ðŸ““[Project documentation](https://docs.google.com/document/d/1Ms0QsfVngobfWqt56lBRlsSpXwqDYske70nw33osBbk/edit?tab=t.0#bookmark=id.6pcek6ioj1hn)\n","\n","ðŸ“‚[Output files](https://drive.google.com/drive/folders/1lpd0RMoZgPh7hFrCFQZ5xH0y1Dpwydp6)\n","\n","\n","================================================================================\n","\n","ðŸ«‚Team 4 members\n","\n","\n","```\n","Christopher Joshua Duran\n","Elsie Reyes\n","Rizza Clare Rozul\n","Bianca Nicole Robles\n","Randolf Blue Araniego III\n","Mark Spencer Conde\n","Luis Montoro\n","Ellaine Maurice Calindas\n","Alisson Villanueva\n","Sophia Melanie Castillo\n","Soleil Francisco\n","Jericho Ramos\n","Bernice Navarro\n","```\n"],"metadata":{"id":"-VPgY0lOV4mn"}},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 1** âœ¨"],"metadata":{"id":"F7F3d5hLyeS_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLFbNbizvFIq","executionInfo":{"status":"error","timestamp":1758779911358,"user_tz":-480,"elapsed":8052,"user":{"displayName":"Mark Anthony Vale","userId":"06836060745688212440"}},"outputId":"a4f610f1-5d66-49ef-9820-7fcf83179a41","colab":{"base_uri":"https://localhost:8080/","height":393}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'docx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1940282660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# For exporting reports to Microsoft Word files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRGBColor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWD_PARAGRAPH_ALIGNMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Library installations\n","\n","# <Uncomment this if running is backend> !pip install python-docx\n","# <Uncomment this if running is backend> !pip install pandas\n","# <Uncomment this if running is backend> !pip install xlsxwriter\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# For sentiment analysis\n","from textblob import TextBlob\n","\n","# For word clouds\n","from wordcloud import WordCloud\n","\n","# For topic analysis\n","from collections import Counter\n","import re\n","\n","# Set plot style\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette(\"husl\")\n","\n","# Set display options\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_colwidth', 50)\n","\n","# For including realtime date elements in output filenames\n","from datetime import date, timedelta\n","\n","# For exporting reports to Microsoft Word files\n","from docx import Document\n","from docx.shared import Pt, Inches, RGBColor\n","from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n","\n","# For file directory references\n","import os, sys"]},{"cell_type":"code","source":["# #ADDED-PART - Prompt the user to key in the file directory path of the source file\n","\n","# TO RUN FROM THE BACKEND: Comment the two lines below\n","source_file_directory_path = input(\"Enter the directory path where the file for processing will come from and where the output files will be saved:\")\n","source_file_directory_path_rfrmt = source_file_directory_path.replace(\"\\\\\", \"/\")\n","\n","# Load the dataset\n","df = pd.read_excel(f'{source_file_directory_path_rfrmt}/Cyclones.xlsx', 'Export')\n","# <Uncomment this if running is backend> <support files> df = pd.read_excel('Cyclones.xlsx', 'Export')\n","# <Uncomment this if running is backend> print(f\"Initial dataset shape: {df.shape}\")\n","# <Uncomment this if running is backend> print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n","\n","# #ADDED-PART - Make a dataframe for the cyclone names and dates\n","# TO RUN FROM THE BACKEND: Comment the line below\n","indx = pd.read_excel(f'{source_file_directory_path_rfrmt}/Cyclones.xlsx', \"Cyclone Name and Dates\", header=[1], usecols = [1, 2, 3])\n","# <Uncomment this if running is backend> <support files> indx = pd.read_excel('Cyclones.xlsx', \"Cyclone Name and Dates\", header=[1], usecols = [1, 2, 3])\n","\n","\n","# #ADDED-PART - Make a dataframe for the source name index to be used in standarizing source names\n","# TO RUN FROM THE BACKEND: Comment the line below\n","sourceindx = pd.read_excel(f'{source_file_directory_path_rfrmt}/Cyclones.xlsx', \"Source Name Index\", header=[1], usecols = [1, 2])\n","# <Uncomment this if running is backend> <support files> sourceindx = pd.read_excel('Cyclones.xlsx', \"Source Name Index\", header=[1], usecols = [1, 2])\n","\n","# #ADDED-PART - Defining datetoday variable to aid templated figure and file titles\n","datetoday = date.today()"],"metadata":{"id":"FadVeZ6ZvQxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for missing values\n","missing_summary = pd.DataFrame({\n","    'Column': df.columns,\n","    'Missing_Count': df.isnull().sum(),\n","    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n","})\n","missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n","# <Uncomment this if running is backend> print(\"Missing Values Summary:\")\n","# <Uncomment this if running is backend> print(missing_summary)\n"],"metadata":{"id":"URJz83yJwIU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store initial count\n","initial_count = len(df)\n","\n","# Handle duplicates\n","# <Uncomment this if running is backend> print(f\"Checking for duplicates...\")\n","\n","# Check duplicates by URL\n","url_duplicates = df.duplicated(subset=['URL'], keep='first').sum()\n","# <Uncomment this if running is backend> print(f\"Duplicates by URL: {url_duplicates}\")\n","\n","# Check duplicates by Headline\n","headline_duplicates = df.duplicated(subset=['Headline'], keep='first').sum()\n","# <Uncomment this if running is backend> print(f\"Duplicates by Headline: {headline_duplicates}\")\n","\n","# Check duplicates by URL and Headline combination\n","combined_duplicates = df.duplicated(subset=['URL', 'Headline'], keep='first').sum()\n","# <Uncomment this if running is backend> print(f\"Duplicates by URL & Headline: {combined_duplicates}\")\n","\n","# Remove duplicates based on URL (keeping first occurrence)\n","df = df.drop_duplicates(subset=['URL'], keep='first')\n","# <Uncomment this if running is backend> print(f\"\\nRecords after removing URL duplicates: {len(df)}\")\n","# <Uncomment this if running is backend> print(f\"Removed {initial_count - len(df)} duplicate records\")"],"metadata":{"id":"4SGpBbfRwNy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handle Date column - Convert to datetime and create date components\n","# <Uncomment this if running is backend> print(\"Processing dates...\")\n","\n","# Convert Date column to datetime\n","df['Publication Date'] = pd.to_datetime(df['Publication Date'], errors='coerce')\n","\n","# Create date component columns\n","df['Year'] = df['Publication Date'].dt.year\n","df['Month'] = df['Publication Date'].dt.month\n","df['Day'] = df['Publication Date'].dt.day\n","df['DayOfWeek'] = df['Publication Date'].dt.day_name()\n","df['MonthName'] = df['Publication Date'].dt.month_name()\n","\n","# Check for any date parsing issues\n","date_issues = df[df['Publication Date'].isnull()]\n","# <Uncomment this if running is backend> print(f\"Records with date parsing issues: {len(date_issues)}\")\n","\n","# <Uncomment this if running is backend> if len(date_issues) > 0:\n","    # <Uncomment this if running is backend> print(\"Sample of problematic dates:\")\n","    # <Uncomment this if running is backend> print(date_issues[['S/N', 'Publication Date']].head())"],"metadata":{"id":"WiCJ7lnVwQ3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #ADDED-PART - Source name standardization\n","sourcemapping = dict(zip(sourceindx[\"Raw Source Name\"], sourceindx[\"Standardized Source Name\"]))\n","df[\"Source\"] = df[\"Source\"].replace(sourcemapping)\n","\n","# #ADDED-PART - Source name column cleaning for Reddit\n","dummyreddit = pd.DataFrame({\n","    'Original Data': [r'(?<=\\w)reddit.com(?=\\w)'],\n","    'Correct Data': ['Reddit']\n","})\n","\n","redditmapping = dict(zip(dummyreddit[\"Original Data\"], dummyreddit[\"Correct Data\"]))\n","df[\"Source\"] = df[\"Source\"].replace(redditmapping)\n","\n","# #ADDED-PART - Remove â€œ(Licensed by Copyright Agency)â€ in source names\n","dummydropvalues = pd.DataFrame({\n","    'Drop Values': [\"(Licensed by Copyright Agency)\", \" (Licensed by Copyright Agency)\"],\n","    'Replacement Values': [\"\", \"\"]\n","})\n","\n","dropvalmap = dict(zip(dummydropvalues[\"Drop Values\"], dummydropvalues[\"Replacement Values\"]))\n","df[\"Source\"] = df[\"Source\"].replace(dropvalmap)\n","\n"],"metadata":{"id":"SWs0tYffyL99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter out irrelevant records\n","# <Uncomment this if running is backend> print(\"Filtering irrelevant records...\")\n","\n","# Remove records with missing critical fields\n","critical_fields = ['Publication Date', 'Headline', 'Source']\n","before_filter = len(df)\n","\n","for field in critical_fields:\n","    df = df[df[field].notna()]\n","\n","# Remove records with very short headlines (likely test data)\n","df = df[df['Headline'].str.len() > 10]\n","\n","# Remove records with invalid dates\n","df = df[df['Year'].notna()]\n","\n","# #ADDED-PART - Remove rows with headlines that indicate invalid data entries\n","\n","invalid_entries = ['This post is unavailable because the account owner limits who can view their posts', 'This account doesnâ€™t exist']\n","\n","invalid_entries_pattern = '|'.join(invalid_entries)\n","\n","df = df[~df[\"Headline\"].str.contains(invalid_entries_pattern, case=False, na=False)]\n","\n","# <Uncomment this if running is backend> print(f\"Records removed by filtering: {before_filter - len(df)}\")\n","# <Uncomment this if running is backend> print(f\"Final cleaned dataset size: {len(df)}\")\n"],"metadata":{"id":"y61BTpVXyRDw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 2** âœ¨"],"metadata":{"id":"HvLT74yZytsd"}},{"cell_type":"code","source":["# Analyze existing Sentiment column\n","# <Uncomment this if running is backend> print(\"Existing Sentiment Distribution:\")\n","existing_sentiment = df['Sentiment'].value_counts()\n","# <Uncomment this if running is backend> print(existing_sentiment)\n","# <Uncomment this if running is backend> print(f\"\\nRecords with sentiment: {df['Sentiment'].notna().sum()}\")\n","# <Uncomment this if running is backend> print(f\"Records without sentiment: {df['Sentiment'].isna().sum()}\")"],"metadata":{"id":"TapJoJxXyxR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define sentiment analysis function using TextBlob\n","def get_sentiment(text):\n","    \"\"\"Analyze sentiment of text using TextBlob\"\"\"\n","    try:\n","        blob = TextBlob(str(text))\n","        polarity = blob.sentiment.polarity\n","\n","        if polarity > 0.1:\n","            return 'Positive'\n","        elif polarity < -0.1:\n","            return 'Negative'\n","        else:\n","            return 'Neutral'\n","    except:\n","        return 'Neutral'\n","\n","# Apply sentiment analysis to headlines\n","# <Uncomment this if running is backend> print(\"Analyzing sentiment of headlines...\")\n","df['Auto_Sentiment'] = df['Headline'].apply(get_sentiment)\n","\n","# Show distribution of auto-generated sentiment\n","# <Uncomment this if running is backend> print(\"\\nAuto-generated Sentiment Distribution:\")\n","auto_sentiment = df['Auto_Sentiment'].value_counts()\n","# <Uncomment this if running is backend> print(auto_sentiment)"],"metadata":{"id":"w2eDo-YayzqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare manual vs automated sentiment\n","# Only compare where manual sentiment exists\n","comparison_df = df[df['Sentiment'].notna()].copy()\n","\n","# Create comparison\n","comparison_df['Match'] = comparison_df['Sentiment'] == comparison_df['Auto_Sentiment']\n","\n","# Calculate match statistics\n","total_compared = len(comparison_df)\n","matches = comparison_df['Match'].sum()\n","mismatches = total_compared - matches\n","\n","# <Uncomment this if running is backend> print(f\"Sentiment Comparison Results:\")\n","# <Uncomment this if running is backend> print(f\"Total records compared: {total_compared}\")\n","# <Uncomment this if running is backend> print(f\"Matches: {matches} ({(matches/total_compared)*100:.1f}%)\")\n","# <Uncomment this if running is backend> print(f\"Mismatches: {mismatches} ({(mismatches/total_compared)*100:.1f}%)\")"],"metadata":{"id":"NzWag0x8y2rF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 4** âœ¨"],"metadata":{"id":"2gg6TwOm03QO"}},{"cell_type":"code","source":["# Process Keywords column\n","# <Uncomment this if running is backend> print(\"Analyzing Keywords...\")\n","\n","# Extract all keywords\n","all_keywords = []\n","for keywords in df['Keywords'].dropna():\n","    # Try different delimiters\n","    if ',' in str(keywords):\n","        keyword_list = str(keywords).split(',')\n","    elif ';' in str(keywords):\n","        keyword_list = str(keywords).split(';')\n","    else:\n","        keyword_list = [str(keywords)]\n","\n","    # Clean and add keywords\n","    all_keywords.extend([k.strip() for k in keyword_list if k.strip()])\n","\n","# Count keyword occurrences\n","keyword_counts = Counter(all_keywords)\n","top_keywords = keyword_counts.most_common(10)\n","\n","# <Uncomment this if running is backend> print(f\"Total unique keywords: {len(keyword_counts)}\")\n","# <Uncomment this if running is backend> print(f\"Total keyword occurrences: {len(all_keywords)}\")\n","# <Uncomment this if running is backend> print(\"\\nTop 10 Keywords:\")\n","# <Uncomment this if running is backend> for keyword, count in top_keywords:\n","    # <Uncomment this if running is backend> print(f\"{keyword}: {count}\")"],"metadata":{"id":"2hE0u_gY0k7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process Key Phrases column\n","# <Uncomment this if running is backend> print(\"\\nAnalyzing Key Phrases...\")\n","\n","# Extract all key phrases\n","all_phrases = []\n","for phrases in df['Key Phrases'].dropna():\n","    # Try different delimiters\n","    if ',' in str(phrases):\n","        phrase_list = str(phrases).split(',')\n","    elif ';' in str(phrases):\n","        phrase_list = str(phrases).split(';')\n","    else:\n","        phrase_list = [str(phrases)]\n","\n","    # Clean and add phrases\n","    all_phrases.extend([p.strip() for p in phrase_list if p.strip()])\n","\n","# Count phrase occurrences\n","phrase_counts = Counter(all_phrases)\n","top_phrases = phrase_counts.most_common(10)\n","\n","# <Uncomment this if running is backend> print(f\"Total unique phrases: {len(phrase_counts)}\")\n","# <Uncomment this if running is backend> print(f\"Total phrase occurrences: {len(all_phrases)}\")\n","# <Uncomment this if running is backend> print(\"\\nTop 10 Key Phrases:\")\n","# <Uncomment this if running is backend> for phrase, count in top_phrases:\n","    # <Uncomment this if running is backend> print(f\"{phrase}: {count}\")"],"metadata":{"id":"uB5bQSIn0niK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyze which sources use which keywords most\n","# <Uncomment this if running is backend> print(\"\\nKeyword Usage by Source:\")\n","\n","# Create a mapping of sources to their top keywords\n","source_keywords = {}\n","for idx, row in df.iterrows():\n","    source = row['Source']\n","    keywords_str = row['Keywords']\n","\n","    if pd.notna(keywords_str) and pd.notna(source):\n","        # Extract keywords for this row\n","        if ',' in str(keywords_str):\n","            keywords = [k.strip() for k in str(keywords_str).split(',')]\n","        elif ';' in str(keywords_str):\n","            keywords = [k.strip() for k in str(keywords_str).split(';')]\n","        else:\n","            keywords = [str(keywords_str).strip()]\n","\n","        # Add to source mapping\n","        if source not in source_keywords:\n","            source_keywords[source] = []\n","        source_keywords[source].extend(keywords)\n","\n","# Get top keywords for top 5 sources\n","top_5_sources = df['Source'].value_counts().head(5).index\n","\n","# <Uncomment this if running is backend> print(\"\\nTop Keywords by Top 5 Sources:\")\n","# <Uncomment this if running is backend> for source in top_5_sources:\n","    # <Uncomment this if running is backend> if source in source_keywords:\n","        # <Uncomment this if running is backend> source_keyword_counts = Counter(source_keywords[source])\n","        # <Uncomment this if running is backend> top_3_keywords = source_keyword_counts.most_common(3)\n","        # <Uncomment this if running is backend> print(f\"\\n{source}:\")\n","        # <Uncomment this if running is backend> for keyword, count in top_3_keywords:\n","            # <Uncomment this if running is backend> print(f\"  - {keyword}: {count}\")"],"metadata":{"id":"QwXGhCQz0t1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 5** âœ¨"],"metadata":{"id":"Jlt_zZlo1Mlo"}},{"cell_type":"code","source":["# Prepare text for word clouds\n","# Combine keywords and key phrases\n","all_text_keywords = ' '.join(all_keywords)\n","all_text_phrases = ' '.join(all_phrases)\n","combined_text = all_text_keywords + ' ' + all_text_phrases\n","\n","# Create overall word cloud\n","plt.figure(figsize=(15, 8))\n","wordcloud = WordCloud(width=800, height=400, background_color='white',\n","                     colormap='viridis', max_words=100).generate(combined_text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.tight_layout()\n","# Uncomment this if running is backend plt.show()\n","plt.savefig(f\"{datetoday} Keyword Wordcloud.png\")"],"metadata":{"id":"5Eq8rVU41L6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create sentiment-specific word clouds\n","fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n","\n","sentiments = ['Positive', 'Negative', 'Neutral']\n","colors = ['Greens', 'Reds', 'Blues']\n","\n","for idx, (sentiment, color) in enumerate(zip(sentiments, colors)):\n","    # Get keywords for this sentiment\n","    sentiment_df = df[df['Auto_Sentiment'] == sentiment]\n","    sentiment_keywords = []\n","\n","    for keywords in sentiment_df['Keywords'].dropna():\n","        if ',' in str(keywords):\n","            keyword_list = str(keywords).split(',')\n","        elif ';' in str(keywords):\n","            keyword_list = str(keywords).split(';')\n","        else:\n","            keyword_list = [str(keywords)]\n","        sentiment_keywords.extend([k.strip() for k in keyword_list if k.strip()])\n","\n","    if sentiment_keywords:\n","        sentiment_text = ' '.join(sentiment_keywords)\n","        wordcloud = WordCloud(width=400, height=300, background_color='white',\n","                            colormap=color, max_words=50).generate(sentiment_text)\n","        axes[idx].imshow(wordcloud, interpolation='bilinear')\n","        axes[idx].axis('off')\n","        axes[idx].set_title(f'{sentiment} Sentiment Word Cloud', fontsize=21)\n","    else:\n","        axes[idx].text(0.5, 0.5, 'No keywords available',\n","                      horizontalalignment='center', verticalalignment='center')\n","        axes[idx].axis('off')\n","        axes[idx].set_title(f'{sentiment} Sentiment Word Cloud', fontsize=21)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{datetoday} Sentiment Word Cloud.png\")\n","# Uncomment this if running is backend plt.show()"],"metadata":{"id":"APxnaTfF1cuo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 6** âœ¨"],"metadata":{"id":"jY3gS8rN1qUs"}},{"cell_type":"code","source":["# Question 1: Top 5 sources by number of stories\n","top_5_sources_list = df['Source'].value_counts().head(5)\n","# <Uncomment this if running is backend> print(\"1. Top 5 Sources by Number of Stories:\")\n","# <Uncomment this if running is backend> print(\"=\"*50)\n","# <Uncomment this if running is backend> for i, (source, count) in enumerate(top_5_sources_list.items(), 1):\n","    # <Uncomment this if running is backend> print(f\"{i}. {source}: {count} stories\")"],"metadata":{"id":"tEgcb3EJffc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question 2: Country with most positive sentiment stories\n","positive_by_country = df[df['Auto_Sentiment'] == 'Positive']['Country'].value_counts()\n","# <Uncomment this if running is backend> print(\"2. Country with Most Positive Sentiment Stories:\")\n","# <Uncomment this if running is backend> print(\"=\"*50)\n","# <Uncomment this if running is backend> if not positive_by_country.empty:\n","    # <Uncomment this if running is backend> top_positive_country = positive_by_country.index[0]\n","    # <Uncomment this if running is backend> top_positive_count = positive_by_country.iloc[0]\n","    # <Uncomment this if running is backend> print(f\"{top_positive_country}: {top_positive_count} positive stories\")\n","    # <Uncomment this if running is backend> print(\"\\nTop 5 countries by positive stories:\")\n","    # <Uncomment this if running is backend> for country, count in positive_by_country.head(5).items():\n","        # <Uncomment this if running is backend> print(f\"  - {country}: {count}\")\n","# <Uncomment this if running is backend> print()"],"metadata":{"id":"AcDS2u1z1zP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question 3: Most common keyword/topic\n","# <Uncomment this if running is backend> print(\"3. Most Common Keyword/Topic:\")\n","# <Uncomment this if running is backend> print(\"=\"*50)\n","# <Uncomment this if running is backend> if top_keywords:\n","    # <Uncomment this if running is backend> most_common_keyword = top_keywords[0]\n","    # <Uncomment this if running is backend> print(f\"Most common keyword: '{most_common_keyword[0]}' (appears {most_common_keyword[1]} times)\")\n","    # <Uncomment this if running is backend> print(\"\\nTop 5 keywords:\")\n","    # <Uncomment this if running is backend> for keyword, count in top_keywords[:5]:\n","        # <Uncomment this if running is backend> print(f\"  - {keyword}: {count}\")\n","# <Uncomment this if running is backend> print()"],"metadata":{"id":"Tg0ZmK8U11kC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question 5: Source with highest average reach\n","# Calculate average total reach by source\n","\n","# Calculate total reach\n","df['Total_Reach'] = df['Desktop Reach'].fillna(0) + df['Mobile Reach'].fillna(0)\n","\n","avg_reach_by_source = df.groupby('Source')['Total_Reach'].agg(['mean', 'count'])\n","avg_reach_by_source = avg_reach_by_source[avg_reach_by_source['count'] >= 5]  # Filter sources with at least 5 stories\n","avg_reach_by_source = avg_reach_by_source.sort_values('mean', ascending=False)\n","\n","# <Uncomment this if running is backend> print(\"5. Source with Highest Average Reach:\")\n","# <Uncomment this if running is backend> print(\"=\"*50)\n","if not avg_reach_by_source.empty:\n","    top_reach_source = avg_reach_by_source.index[0]\n","    top_reach_value = avg_reach_by_source.iloc[0]['mean']\n","    top_reach_count = avg_reach_by_source.iloc[0]['count']\n","    # <Uncomment this if running is backend> print(f\"{top_reach_source}: {top_reach_value:,.0f} average reach (from {top_reach_count} stories)\")\n","    # <Uncomment this if running is backend> print(\"\\nTop 5 sources by average reach (min 5 stories):\")\n","    for source in avg_reach_by_source.head(5).index:\n","        avg = avg_reach_by_source.loc[source, 'mean']\n","        count = avg_reach_by_source.loc[source, 'count']\n","        # <Uncomment this if running is backend> print(f\"  - {source}: {avg:,.0f} (from {count} stories)\")\n","# <Uncomment this if running is backend> print()"],"metadata":{"id":"-DoFkdJk15kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3 Interesting Findings & Recommendations\n","# <Uncomment this if running is backend> print(\"\\n\" + \"=\"*70)\n","# <Uncomment this if running is backend> print(\"3 INTERESTING FINDINGS:\")\n","# <Uncomment this if running is backend> print(\"=\"*70)\n","\n","# Finding 1: Sentiment distribution\n","sentiment_pct = df['Auto_Sentiment'].value_counts(normalize=True) * 100\n","# <Uncomment this if running is backend> print(f\"1. Sentiment Balance: {sentiment_pct.get('Positive', 0):.1f}% positive, \"\n","      # <Uncomment this if running is backend> f\"{sentiment_pct.get('Negative', 0):.1f}% negative, \"\n","      # <Uncomment this if running is backend> f\"{sentiment_pct.get('Neutral', 0):.1f}% neutral coverage\")\n","\n","# Finding 2: Coverage concentration\n","top_3_sources_pct = (top_5_sources_list.head(3).sum() / len(df)) * 100\n","# <Uncomment this if running is backend> print(f\"2. Source Concentration: Top 3 sources account for {top_3_sources_pct:.1f}% of all coverage\")\n","\n","# Finding 3: Geographic focus\n","top_country_pct = (df['Country'].value_counts().iloc[0] / len(df)) * 100\n","top_country_name = df['Country'].value_counts().index[0]\n","# <Uncomment this if running is backend> print(f\"3. Geographic Focus: {top_country_name} dominates with {top_country_pct:.1f}% of all stories\")\n","\n","# <Uncomment this if running is backend> print(\"\\n\" + \"=\"*70)\n","# <Uncomment this if running is backend> print(\"RECOMMENDATIONS:\")\n","# <Uncomment this if running is backend> print(\"=\"*70)\n","# <Uncomment this if running is backend> print(\"Based on the analysis, here are key recommendations for media monitoring:\")\n","# <Uncomment this if running is backend> print(\"\\n1. DIVERSIFY SOURCES: Consider monitoring more diverse news sources to reduce\")\n","# <Uncomment this if running is backend> print(\"   concentration risk and get broader perspectives.\")\n","# <Uncomment this if running is backend> print(\"\\n2. FOCUS ON HIGH-REACH CONTENT: Prioritize monitoring and engagement with\")\n","# <Uncomment this if running is backend> print(f\"   sources like {avg_reach_by_source.index[0]} that have higher average reach.\")\n","# <Uncomment this if running is backend> print(\"\\n3. SENTIMENT TRACKING: Implement real-time sentiment monitoring to quickly\")\n","# <Uncomment this if running is backend> print(\"   identify and respond to negative coverage trends.\")\n","# <Uncomment this if running is backend> print(\"\\n4. KEYWORD OPTIMIZATION: Focus content strategy around top keywords like\")\n","# <Uncomment this if running is backend> print(f\"   '{top_keywords[0][0]}' which appear most frequently in coverage.\")\n","# <Uncomment this if running is backend> print(\"=\"*70)"],"metadata":{"id":"rR-dVLi41-mm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 7** âœ¨"],"metadata":{"id":"UMZmADZj4sPV"}},{"cell_type":"code","source":["# Prepare clean dataset for export\n","# #MODIFICATION - Select relevant columns for export (added 'Opening Text', 'Hit Sentence')\n","export_columns = ['Publication Date', 'Year', 'Month', 'Day', 'Headline', 'URL', 'Opening Text', 'Hit Sentence',\n","                 'Source', 'Country', 'Sentiment', 'Auto_Sentiment',\n","                 'Keywords', 'Key Phrases', 'Total_Reach', 'Desktop Reach', 'Mobile Reach']\n","\n","clean_export_df = df[export_columns].copy()\n","\n","# Create summary dataframes\n","# Summary statistics\n","summary_stats = pd.DataFrame({\n","    'Metric': ['Total Stories', 'Date Range', 'Unique Sources', 'Unique Countries',\n","               'Stories with Keywords', 'Average Total Reach'],\n","    'Value': [\n","        len(df),\n","        f\"{df['Publication Date'].min().date()} to {df['Publication Date'].max().date()}\",\n","        df['Source'].nunique(),\n","        df['Country'].nunique(),\n","        df['Keywords'].notna().sum(),\n","        f\"{df['Total_Reach'].mean():,.0f}\"\n","    ]\n","})\n","\n","# Top sources summary\n","top_sources_summary = pd.DataFrame({\n","    'Source': top_5_sources_list.index,\n","    'Story_Count': top_5_sources_list.values,\n","    'Percentage': (top_5_sources_list.values / len(df) * 100).round(1)\n","})\n","\n","# Top keywords summary\n","top_keywords_summary = pd.DataFrame(top_keywords[:10], columns=['Keyword', 'Count'])\n","\n","# Sentiment summary\n","sentiment_counts = df['Auto_Sentiment'].value_counts()\n","sentiment_summary = pd.DataFrame({\n","    'Sentiment': sentiment_counts.index,\n","    'Count': sentiment_counts.values,\n","    'Percentage': (sentiment_counts.values / sentiment_counts.sum() * 100).round(1)\n","})"],"metadata":{"id":"rI48t_kW2B2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨ **CHALLENGE 8** âœ¨"],"metadata":{"id":"6EGjDoL0fYV-"}},{"cell_type":"code","source":["# ADDED-PART - Creating dataframes counting the cyclone mentions in the news headlines, opening texts, and hit sentences\n","\n","# ADDED-PART - Make an array for the list of the year's tropical cyclones based on PAGASA records\n","cyclone_list = indx['Cyclone Name'].tolist()\n","\n","# ADDED-PART - Create a regex pattern to search for any of the cyclone names in headlines\n","cyclone_pattern = '|'.join(cyclone_list)\n","\n","# ADDED-PART - Making an array of the columns to search\n","column_mentions = [\"Headline\", \"Opening Text\", \"Hit Sentence\"]\n","\n","# ADDED-PART - Create an empty dataframe for results\n","results = pd.DataFrame(0, index=cyclone_list, columns=column_mentions)\n","\n","# ADDED-PART - Filter the dataset to only include records where headline, opening text, and hit sentence columns contain cyclone names\n","for kw in cyclone_list:\n","    # Build regex pattern for the single keyword\n","    pattern = rf\"\\b{re.escape(kw)}\\b\"   # \\b ensures whole word match, re.escape protects special chars\n","\n","    for col in column_mentions:\n","        # Boolean mask per row: did this keyword appear at least once in this cell?\n","        mask = df[col].str.contains(pattern, case=False, na=False)\n","\n","        # Count unique appearances (one per row, per column)\n","        results.loc[kw, col] = mask.sum()\n","\n","# <Uncomment this if running is backend> print(results)\n"],"metadata":{"id":"tj-upTjHfb-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make sure dates are datetime\n","df[\"Publication Date\"] = pd.to_datetime(df[\"Publication Date\"])\n","indx[\"PAR Entry\"] = pd.to_datetime(indx[\"PAR Entry\"])\n","indx[\"PAR Exit\"] = pd.to_datetime(indx[\"PAR Exit\"])\n","\n","# ADDED-PART - Creating dataframes counting the cyclone mentions in the news headlines, opening texts, and hit sentences\n","# This is with consideration of the active period and aftermath period dates indicated in the dataframe called indx\n","def count_keyword_mentions(df, indx, target_col):\n","    results = []\n","\n","    for _, row in indx.iterrows():\n","        keyword = row[\"Cyclone Name\"]\n","        start_date = row[\"PAR Entry\"]\n","        end_date = row[\"PAR Exit\"]\n","\n","        # regex pattern (whole word match, case-insensitive)\n","        pattern = rf\"\\b{re.escape(keyword)}\\b\"\n","\n","        # Rows where the keyword appears in target_col\n","        mask = df[target_col].str.contains(pattern, case=False, na=False)\n","\n","        # Within date range\n","        within_range = df.loc[mask & (df[\"Publication Date\"].between(start_date, end_date))]\n","        count_within = within_range.shape[0]\n","\n","        # After end date\n","        after_range = df.loc[mask & (df[\"Publication Date\"] > end_date)]\n","        count_after = after_range.shape[0]\n","\n","        # Sum of active period and aftermath per typhoon\n","        activeplusafter = count_within + count_after\n","\n","        results.append([keyword, count_within, count_after, activeplusafter])\n","\n","    # Return as dataframe\n","    return pd.DataFrame(results, columns=[\"Typhoon\", \"Active Period\", \"Aftermath\", \"Total\"])\n","\n","# ADDED-PART - Generate tally tables\n","table1 = count_keyword_mentions(df, indx, \"Headline\")\n","table2 = count_keyword_mentions(df, indx, \"Opening Text\")\n","table3 = count_keyword_mentions(df, indx, \"Hit Sentence\")\n","\n","# ADDED-PART - Example printout\n","# <Uncomment this if running is backend> print(\"Table 1 - Headline Mentions\")\n","# <Uncomment this if running is backend> print(table1)\n","# <Uncomment this if running is backend> print(\"\\nTable 2 - Opening Text Mentions\")\n","# <Uncomment this if running is backend> print(table2)\n","# <Uncomment this if running is backend> print(\"\\nTable 3 - Hit Sentence Mentions\")\n","# <Uncomment this if running is backend> print(table3)\n"],"metadata":{"id":"cpyiOfcCGbL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MODIFICATION - Create a stacked bar chart to visualize the data, TABLE 1 (HEADLINE)\n","plt.figure(figsize=(12, 7))  # Set figure size for better readability\n","\n","# MODIFICATION - Filter rows where either Start or End > 0\n","partialfiltered_table1 = table1[(table1['Active Period'] != 0) & (table1['Aftermath'] != 0)]\n","furtherfiltered_table1 = partialfiltered_table1.assign(total=partialfiltered_table1['Active Period']+partialfiltered_table1['Aftermath'])\n","filtered_table1 = furtherfiltered_table1.sort_values(by = 'Total', ascending = False)\n","\n","# MODIFICATION - Plot the stacked bars\n","plt.bar(filtered_table1['Typhoon'], filtered_table1['Active Period'], label='Active Period', color='#ff6b6b', alpha=0.8)\n","plt.bar(filtered_table1['Typhoon'], filtered_table1['Aftermath'], bottom=filtered_table1['Active Period'], label='Aftermath', color='#4ecdc4', alpha=0.8)\n","\n","# MODIFICATION - Customize the chart appearance\n","plt.title('Typhoon Media Coverage: Active Period vs Aftermath (Headline Mentions)',\n","          fontsize=16, fontweight='bold', pad=20)\n","plt.xlabel('Typhoon Name', fontsize=12, fontweight='bold')\n","plt.ylabel('Number of Mentions', fontsize=12, fontweight='bold')\n","plt.legend(fontsize=11)  # Add legend to distinguish between active period and aftermath\n","plt.xticks(rotation=45, fontsize=11)  # Rotate x-axis labels for better readability\n","plt.yticks(fontsize=11)\n","\n","# MODIFICATION - Add value labels on top of each bar segment for clarity\n","for i, (active, aftermath) in enumerate(zip(filtered_table1['Active Period'], filtered_table1['Aftermath'])):\n","\n","    # Add label for active period segment (if > 0)\n","    if active > 0:\n","        plt.text(i, active/2, str(active), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","    # Add label for aftermath segment (if > 0)\n","    if aftermath > 0:\n","        plt.text(i, active + aftermath/2, str(aftermath), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","# MODIFICATION - Add subtle grid lines for easier reading\n","plt.grid(axis='y', alpha=0.3)\n","plt.tight_layout()  # Adjust layout to prevent label cutoff\n","plt.savefig(f\"{datetoday} Headline Mentions.png\") # Or \"my_plot.svg\" or \"my_plot.pdf\"\n","# Uncomment this if running is backend plt.show() # Display the chart\n","\n","\n","# MODIFICATION - Create a stacked bar chart to visualize the data, TABLE 2 (OPENING TEXT)\n","plt.figure(figsize=(12, 7))  # Set figure size for better readability\n","\n","# MODIFICATION - Filter rows where either Start or End > 0\n","partialfiltered_table2 = table2[(table2['Active Period'] != 0) & (table2['Aftermath'] != 0)]\n","furtherfiltered_table2 = partialfiltered_table2.assign(total=partialfiltered_table2['Active Period']+partialfiltered_table2['Aftermath'])\n","filtered_table2 = furtherfiltered_table2.sort_values(by = 'Total', ascending = False)\n","\n","# MODIFICATION - Plot the stacked bars\n","plt.bar(filtered_table2['Typhoon'], filtered_table2['Active Period'], label='Active Period', color='#ff6b6b', alpha=0.8)\n","plt.bar(filtered_table2['Typhoon'], filtered_table2['Aftermath'], bottom=filtered_table2['Active Period'], label='Aftermath', color='#4ecdc4', alpha=0.8)\n","\n","# MODIFICATION - Customize the chart appearance\n","plt.title('Typhoon Media Coverage: Active Period vs Aftermath (Opening Text Mentions)',\n","          fontsize=16, fontweight='bold', pad=20)\n","plt.xlabel('Typhoon Name', fontsize=12, fontweight='bold')\n","plt.ylabel('Number of Mentions', fontsize=12, fontweight='bold')\n","plt.legend(fontsize=11)  # Add legend to distinguish between active period and aftermath\n","plt.xticks(rotation=45, fontsize=11)  # Rotate x-axis labels for better readability\n","plt.yticks(fontsize=11)\n","\n","# MODIFICATION - Add value labels on top of each bar segment for clarity\n","for i, (active, aftermath) in enumerate(zip(filtered_table2['Active Period'], filtered_table2['Aftermath'])):\n","\n","    # Add label for active period segment (if > 0)\n","    if active > 0:\n","        plt.text(i, active/2, str(active), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","    # Add label for aftermath segment (if > 0)\n","    if aftermath > 0:\n","        plt.text(i, active + aftermath/2, str(aftermath), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","# MODIFICATION - Add subtle grid lines for easier reading\n","plt.grid(axis='y', alpha=0.3)\n","plt.tight_layout()  # Adjust layout to prevent label cutoff\n","plt.savefig(f\"{datetoday} Opening Text Mentions.png\") # Or \"my_plot.svg\" or \"my_plot.pdf\"\n","# Uncomment this if running is backend plt.show()  # Display the chart\n","\n","\n","# MODIFICATION - Create a stacked bar chart to visualize the data, TABLE 3 (HIT SENTENCE)\n","plt.figure(figsize=(12, 7))  # Set figure size for better readability\n","\n","# MODIFICATION - Filter rows where either Start or End > 0\n","partialfiltered_table3 = table3[(table3['Active Period'] != 0) & (table3['Aftermath'] != 0)]\n","furtherfiltered_table3 = partialfiltered_table3.assign(total=partialfiltered_table3['Active Period']+partialfiltered_table3['Aftermath'])\n","filtered_table3 = furtherfiltered_table3.sort_values(by = 'Total', ascending = False)\n","\n","# MODIFICATION - Plot the stacked bars\n","plt.bar(filtered_table3['Typhoon'], filtered_table3['Active Period'], label='Active Period', color='#ff6b6b', alpha=0.8)\n","plt.bar(filtered_table3['Typhoon'], filtered_table3['Aftermath'], bottom=filtered_table3['Active Period'], label='Aftermath', color='#4ecdc4', alpha=0.8)\n","\n","# MODIFICATION - Customize the chart appearance\n","plt.title('Typhoon Media Coverage: Active Period vs Aftermath (Hit Sentence Mentions)',\n","          fontsize=16, fontweight='bold', pad=20)\n","plt.xlabel('Typhoon Name', fontsize=12, fontweight='bold')\n","plt.ylabel('Number of Mentions', fontsize=12, fontweight='bold')\n","plt.legend(fontsize=11)  # Add legend to distinguish between active period and aftermath\n","plt.xticks(rotation=45, fontsize=11)  # Rotate x-axis labels for better readability\n","plt.yticks(fontsize=11)\n","\n","# MODIFICATION - Add value labels on top of each bar segment for clarity\n","for i, (active, aftermath) in enumerate(zip(filtered_table3['Active Period'], filtered_table3['Aftermath'])):\n","\n","    # Add label for active period segment (if > 0)\n","    if active > 0:\n","        plt.text(i, active/2, str(active), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","    # Add label for aftermath segment (if > 0)\n","    if aftermath > 0:\n","        plt.text(i, active + aftermath/2, str(aftermath), ha='center', va='baseline',\n","                fontweight='bold', color='gray', fontsize=10)\n","\n","# MODIFICATION - Add subtle grid lines for easier reading\n","plt.grid(axis='y', alpha=0.3)\n","plt.tight_layout()  # Adjust layout to prevent label cutoff\n","plt.savefig(f\"{datetoday} Hit Sentence Mentions.png\") # Or \"my_plot.svg\" or \"my_plot.pdf\"\n","# Uncomment this if running is backend plt.show()  # Display the chart\n","\n"],"metadata":{"id":"-H4pJXCHqR_h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ¨**Exporting outputs to files** âœ¨"],"metadata":{"id":"7J6mJ8AtCl3s"}},{"cell_type":"code","source":["# MODIFICATION - Create Excel file with multiple sheets\n","\n","#uncomment when building exe with pd.ExcelWriter(source_file_directory_path_rfrmt + f\"/{datetoday} Cyclone Report.xlsx\", engine='openpyxl') as writer:\n","\n","with pd.ExcelWriter(\"Cyclone Report.xlsx\", engine='openpyxl') as writer:\n","\n","    # Sheet 1: Clean Data\n","    clean_export_df.to_excel(writer, sheet_name='Clean_Data', index=False)\n","\n","    #  MODIFICATION - Sheet 2: Summary\n","    summary_stats.to_excel(writer, sheet_name='Summary', index=False, startrow=0)\n","    writer.sheets['Summary'].cell(row=len(summary_stats)+3, column=1).value = 'Top Sources'\n","    top_sources_summary.to_excel(writer, sheet_name='Summary', index=False,\n","                                startrow=len(summary_stats)+4)\n","\n","    writer.sheets['Summary'].cell(row=len(summary_stats)+len(top_sources_summary)+7,\n","                                 column=1).value = 'Top Keywords'\n","    top_keywords_summary.to_excel(writer, sheet_name='Summary', index=False,\n","                                 startrow=len(summary_stats)+len(top_sources_summary)+8)\n","\n","    writer.sheets['Summary'].cell(row=len(summary_stats)+len(top_sources_summary)+len(top_keywords_summary)+11,\n","                                 column=1).value = 'Sentiment Distribution'\n","    sentiment_summary.to_excel(writer, sheet_name='Summary', index=False,\n","                              startrow=len(summary_stats)+len(top_sources_summary)+len(top_keywords_summary)+12)\n","    summary_sheet = writer.sheets['Summary']\n","\n","    #  ADDED-PART - Sheet 3: Source name index and unique sources in the current dataset\n","\n","    sourceindx.to_excel(writer, sheet_name='Source Indexes', index=False, startrow=1, startcol=1)\n","    sheet3src = writer.sheets['Source Indexes']\n","    usrccurrentdata = pd.DataFrame({'Unique Sources': np.unique(df[\"Source\"])})\n","    sorted_usrccurrentdata = usrccurrentdata.sort_values(by=\"Unique Sources\", ascending=True)\n","    sorted_usrccurrentdata.to_excel(writer, sheet_name='Source Indexes', index=False, index_label=\"Unique Sources In Current Dataset\", startrow=1, startcol=4)\n","\n","    # ADDED-PART - Sheet 4: Cyclone Mentions\n","\n","    def align_left(x):\n","      return ['text-align: left' for x in x]\n","\n","    headline_summary_label = {\"Table 1 - Headline Mentions\":[]}\n","    headline_summary_labeldf = pd.DataFrame(headline_summary_label)\n","    headline_summary_labeldf.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=1, startcol=1)\n","    table1.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=2, startcol=1)\n","    cycmentions = writer.sheets['Cyclone Mentions']\n","    openingtext_summary_label = {\"Table 2 - Opening Text Mentions\":[]}\n","    openingtext_summary_labeldf = pd.DataFrame(openingtext_summary_label)\n","    openingtext_summary_labeldf.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=1, startcol=6)\n","    table2.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=2, startcol=6)\n","    hitsentence_summary_label = {\"Table 3 - Hit Sentence Text Mentions\":[]}\n","    hitsentence_summary_labeldf = pd.DataFrame(hitsentence_summary_label)\n","    hitsentence_summary_labeldf.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=1, startcol=11)\n","    table3.to_excel(writer, sheet_name='Cyclone Mentions', index=False, startrow=2, startcol=11)\n","\n","    # ADDED-PART - Setting column widths\n","\n","    summary_sheet.column_dimensions['A'].width = 20\n","    summary_sheet.column_dimensions['B'].width = 20\n","    summary_sheet.column_dimensions['C'].width = 20\n","    sheet3src.column_dimensions['B'].width = 30\n","    sheet3src.column_dimensions['C'].width = 30\n","    sheet3src.column_dimensions['E'].width = 50\n","    cycmentions.column_dimensions['B'].width = 35\n","    cycmentions.column_dimensions['C'].width = 20\n","    cycmentions.column_dimensions['D'].width = 20\n","    cycmentions.column_dimensions['G'].width = 35\n","    cycmentions.column_dimensions['H'].width = 20\n","    cycmentions.column_dimensions['I'].width = 20\n","    cycmentions.column_dimensions['L'].width = 35\n","    cycmentions.column_dimensions['M'].width = 20\n","    cycmentions.column_dimensions['N'].width = 20\n"],"metadata":{"id":"toDbxDyiCju7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ADDED-PART Save cyclone mention charts in a Microsoft Word file with placeholders for narrative texts\n","\n","# =========================\n","# ADDED-PART Helper: Resource path (for PyInstaller exe later)\n","# =========================\n","\n","def resource_path(relative_path):\n","    # \"\"\" Get absolute path to resource, works for dev and for PyInstaller exe \"\"\"\n","    if hasattr(sys, '_MEIPASS'):  # PyInstaller temp folder\n","        return os.path.join(sys._MEIPASS, relative_path)\n","    return os.path.join(os.path.abspath(\".\"), relative_path)\n","\n","\n","header_path = resource_path(\"header.png\")\n","\n","\n","# =========================\n","# ADDED-PART Helper: format a run\n","# =========================\n","def format_run(run, font_name=\"DejaVu Sans\", size=12, bold=False, color=(128,128,128)):\n","    font = run.font\n","    font.name = font_name\n","    font.size = Pt(size)\n","    font.bold = bold\n","    font.color.rgb = RGBColor(*color)\n","\n","# =========================\n","# ADDED-PART Start Word document\n","# =========================\n","doc = Document()\n","\n","# --- Header image ---\n","section = doc.sections[0]\n","header = section.header\n","p = header.paragraphs[0]\n","run = p.add_run()\n","# Fit header image inside 1-inch margins of Letter size (~6.5 inches wide)\n","# TO RUN FROM THE BACKEND: Comment the line below\n","run.add_picture(resource_path(\"header.png\"), width=Inches(6.5))\n","# <Uncomment this if running is backend> <support files> run.add_picture(\"header.png\", width=Inches(6.5))\n","\n","\n","# --- Report title ---\n","doc.add_paragraph()\n","title_p = doc.add_paragraph()\n","title_p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = title_p.add_run(f\"Cyclone Report for {datetoday}\")\n","format_run(run, size=16, bold=True, color=(128,128,128))\n","\n","# --- Intro text ---\n","intro_p = doc.add_paragraph()\n","intro_p.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = intro_p.add_run(\"This Weekly Cyclone News Reportage Summary is a brief look at the differences in local cyclone news coverage during a typhoon's active period and aftermath. This was done by getting the counts of the cyclones' mentions in news headlines, opening texts, and prominent news sentences. These details were distilled in this document's simple stacked charts and short summaries for the intended readers' perusal. Additionally, word clouds on the reportage's prominent keywords and sentiments were visualized at the end of this document. This report could serve purposes such as informing its end-users about potential decisions impacted by cyclones news coverage (i.e., facilitate more press release for under-covered cyclones).\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =============================================================================================================\n","# ADDED-PART Section 1: Headline Mentions\n","# =============================================================================================================\n","\n","# Section heading\n","doc.add_page_break()\n","doc.add_paragraph()\n","doc.add_paragraph()\n","sec_p = doc.add_paragraph()\n","sec_p.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n","run = sec_p.add_run(\"Cyclone Headline Mentions\")\n","format_run(run, size=14, bold=True, color=(128,128,128))\n","\n","# Insert Headline Mentions bar graph\n","doc.add_picture(f\"{datetoday} Headline Mentions.png\", width=Inches(6))\n","\n","# One line break + figure caption\n","doc.add_paragraph()\n","cap_p = doc.add_paragraph()\n","cap_p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = cap_p.add_run(\"Figure 1. Headline Mentions\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# Line break\n","doc.add_paragraph()\n","\n","# Placeholder texts\n","table = doc.add_table(rows=1, cols=1)\n","cell = table.cell(0,0)\n","cell.text = \"<This is a placeholder for the stacked chart's accompanying narrative.>\"\n","p = cell.paragraphs[0]\n","p.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = p.runs[0]\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =============================================================================================================\n","# ADDED-PART Section 2: Opening Text Mentions\n","# =============================================================================================================\n","\n","# Section heading\n","doc.add_page_break()\n","doc.add_paragraph()\n","doc.add_paragraph()\n","sec_p2 = doc.add_paragraph()\n","sec_p2.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n","run = sec_p2.add_run(\"Cyclone Opening Text Mentions\")\n","format_run(run, size=14, bold=True, color=(128,128,128))\n","\n","# Insert Opening Text Mentions bar graph\n","doc.add_picture(f\"{datetoday} Opening Text Mentions.png\", width=Inches(6))\n","\n","# One line break + figure caption\n","doc.add_paragraph()\n","cap_p2 = doc.add_paragraph()\n","cap_p2.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = cap_p2.add_run(\"Figure 2. Opening Text Mentions\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# Line break\n","doc.add_paragraph()\n","\n","# Placeholder texts\n","table2 = doc.add_table(rows=1, cols=1)\n","cell2 = table2.cell(0,0)\n","cell2.text = \"<This is a placeholder for the stacked chart's accompanying narrative.>\"\n","p2 = cell2.paragraphs[0]\n","p2.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = p2.runs[0]\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =============================================================================================================\n","# ADDED-PART Section 3: Hit Sentence Mentions\n","# =============================================================================================================\n","\n","# Section heading\n","doc.add_page_break()\n","doc.add_paragraph()\n","doc.add_paragraph()\n","sec_p2 = doc.add_paragraph()\n","sec_p2.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n","run = sec_p2.add_run(\"Cyclone Hit Sentence Mentions\")\n","format_run(run, size=14, bold=True, color=(128,128,128))\n","\n","# Insert Opening Text Mentions bar graph\n","doc.add_picture(f\"{datetoday} Hit Sentence Mentions.png\", width=Inches(6))\n","\n","# One line break + figure caption\n","doc.add_paragraph()\n","cap_p2 = doc.add_paragraph()\n","cap_p2.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = cap_p2.add_run(\"Figure 3. Hit Sentence Mentions\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# Line break\n","doc.add_paragraph()\n","\n","# Placeholder texts\n","table2 = doc.add_table(rows=1, cols=1)\n","cell2 = table2.cell(0,0)\n","cell2.text = \"<This is a placeholder for the stacked chart's accompanying narrative.>\"\n","p2 = cell2.paragraphs[0]\n","p2.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = p2.runs[0]\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =============================================================================================================\n","# ADDED-PART Section 4: Keyword wordcloud\n","# =============================================================================================================\n","\n","# Section heading\n","doc.add_page_break()\n","doc.add_paragraph()\n","doc.add_paragraph()\n","sec_p3 = doc.add_paragraph()\n","sec_p3.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n","run = sec_p3.add_run(\"Keywords Word Cloud\")\n","format_run(run, size=14, bold=True, color=(128,128,128))\n","\n","# Insert Keyword Wordcloud bar graph\n","doc.add_picture(f\"{datetoday} Keyword Wordcloud.png\", width=Inches(6))\n","\n","# One line break + figure caption\n","doc.add_paragraph()\n","cap_p3 = doc.add_paragraph()\n","cap_p3.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = cap_p3.add_run(\"Figure 4. News keywords wordcloud\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# Line break\n","doc.add_paragraph()\n","\n","# Placeholder texts\n","table3 = doc.add_table(rows=1, cols=1)\n","cell3 = table3.cell(0,0)\n","cell3.text = \"<This is a placeholder for the keywords word cloud's accompanying narrative.>\"\n","p3 = cell3.paragraphs[0]\n","p3.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = p3.runs[0]\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =============================================================================================================\n","# ADDED-PART Section 5: Sentiment wordcloud\n","# =============================================================================================================\n","\n","# Section heading\n","doc.add_page_break()\n","doc.add_paragraph()\n","doc.add_paragraph()\n","sec_p4 = doc.add_paragraph()\n","sec_p4.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n","run = sec_p4.add_run(\"Sentiment Word Cloud\")\n","format_run(run, size=14, bold=True, color=(128,128,128))\n","\n","# Insert Keyword Wordcloud bar graph\n","doc.add_picture(f\"{datetoday} Sentiment Word Cloud.png\", width=Inches(6))\n","\n","# One line break + figure caption\n","doc.add_paragraph()\n","cap_p4 = doc.add_paragraph()\n","cap_p4.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = cap_p4.add_run(\"Figure 4. Sentiment wordcloud\")\n","format_run(run, size=12, color=(128,128,128))\n","\n","# Line break\n","doc.add_paragraph()\n","\n","# Placeholder texts\n","table4 = doc.add_table(rows=1, cols=1)\n","cell4 = table4.cell(0,0)\n","cell4.text = \"<This is a placeholder for the sentiment word cloud's accompanying narrative.>\"\n","p4 = cell4.paragraphs[0]\n","p4.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n","run = p4.runs[0]\n","format_run(run, size=12, color=(128,128,128))\n","\n","# =========================\n","# ADDED-PART Footer\n","# =========================\n","\n","# Footer message\n","foot_p = doc.add_paragraph()\n","foot_p.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n","run = foot_p.add_run(\"This report was brought by Team 4. For feedback, please email team4@gmail.com\")\n","format_run(run, size=11, color=(128,128,128))\n","\n","# =========================\n","# ADDED-PART Save document\n","# =========================\n","doc.save(f\"{datetoday} Cyclone Report.docx\")\n","\n","\n","\n"],"metadata":{"id":"KABjUIZrDtMQ"},"execution_count":null,"outputs":[]}]}